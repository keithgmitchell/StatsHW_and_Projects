\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {subsection}{Dataset 1: Conduct multiple linear regression;}{1}{section*.1}\protected@file@percent }
\newlabel{dataset-1-conduct-multiple-linear-regression}{{}{1}{Dataset 1: Conduct multiple linear regression;}{section*.1}{}}
\@writefile{toc}{\contentsline {subsection}{Dataset 2: Conduct two-sample test and LDA;}{1}{section*.2}\protected@file@percent }
\newlabel{dataset-2-conduct-two-sample-test-and-lda}{{}{1}{Dataset 2: Conduct two-sample test and LDA;}{section*.2}{}}
\@writefile{toc}{\contentsline {subsection}{Dataset 3: Conduct PCA.}{1}{section*.3}\protected@file@percent }
\newlabel{dataset-3-conduct-pca.}{{}{1}{Dataset 3: Conduct PCA}{section*.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{For each data analysis, you should write in full sentences, and have thefollowing sections for the body of your report.}{1}{section*.4}\protected@file@percent }
\newlabel{for-each-data-analysis-you-should-write-in-full-sentences-and-have-thefollowing-sections-for-the-body-of-your-report.}{{}{1}{For each data analysis, you should write in full sentences, and have thefollowing sections for the body of your report}{section*.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{Details:}{1}{section*.5}\protected@file@percent }
\newlabel{details}{{}{1}{Details:}{section*.5}{}}
\@writefile{toc}{\contentsline {section}{DATASET 1: Multiple Linear Regression}{2}{section*.6}\protected@file@percent }
\newlabel{dataset-1-multiple-linear-regression}{{}{2}{DATASET 1: Multiple Linear Regression}{section*.6}{}}
\@writefile{toc}{\contentsline {subsection}{INTRODUCTION:}{2}{section*.7}\protected@file@percent }
\newlabel{introduction}{{}{2}{INTRODUCTION:}{section*.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{A company is interested in considering the purchase of a computer but first must assess their future needs in order to determine the proper equipment. A computer scientist collected data from seven similar company sites so that a forecast equation of computer-hardware requirements for inventory management could be developed. (pg 380) This is an important concept for companies running web infrastructure to predict for a variety of reasons.}{2}{section*.8}\protected@file@percent }
\newlabel{a-company-is-interested-in-considering-the-purchase-of-a-computer-but-first-must-assess-their-future-needs-in-order-to-determine-the-proper-equipment.-a-computer-scientist-collected-data-from-seven-similar-company-sites-so-that-a-forecast-equation-of-computer-hardware-requirements-for-inventory-management-could-be-developed.-pg-380-this-is-an-important-concept-for-companies-running-web-infrastructure-to-predict-for-a-variety-of-reasons.}{{}{2}{A company is interested in considering the purchase of a computer but first must assess their future needs in order to determine the proper equipment. A computer scientist collected data from seven similar company sites so that a forecast equation of computer-hardware requirements for inventory management could be developed. (pg 380) This is an important concept for companies running web infrastructure to predict for a variety of reasons}{section*.8}{}}
\@writefile{toc}{\contentsline {subsection}{SUMMARIZE DATA: The first colum of our data is the Orders(in thousands), and the Second column is the Add-Delete items (in thousands)}{2}{section*.9}\protected@file@percent }
\newlabel{summarize-data-the-first-colum-of-our-data-is-the-ordersin-thousands-and-the-second-column-is-the-add-delete-items-in-thousands}{{}{2}{SUMMARIZE DATA: The first colum of our data is the Orders(in thousands), and the Second column is the Add-Delete items (in thousands)}{section*.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{Lets summarize our data first and see what the numbers look like.}{2}{section*.10}\protected@file@percent }
\newlabel{lets-summarize-our-data-first-and-see-what-the-numbers-look-like.}{{}{2}{Lets summarize our data first and see what the numbers look like}{section*.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{Now lets visualize these numbers with a box plot of each of the variables, including the one we are trying to predict.}{3}{section*.11}\protected@file@percent }
\newlabel{now-lets-visualize-these-numbers-with-a-box-plot-of-each-of-the-variables-including-the-one-we-are-trying-to-predict.}{{}{3}{Now lets visualize these numbers with a box plot of each of the variables, including the one we are trying to predict}{section*.11}{}}
\@writefile{toc}{\contentsline {subsubsection}{Lets also just get a preliminarly look at what our variables ``Orders(thousands)'' andn ``Add-delete items(thousands)'' look like when graphed against the variable we hope to predict ``CPU time''}{3}{section*.12}\protected@file@percent }
\newlabel{lets-also-just-get-a-preliminarly-look-at-what-our-variables-ordersthousands-andn-add-delete-itemsthousands-look-like-when-graphed-against-the-variable-we-hope-to-predict-cpu-time}{{}{3}{Lets also just get a preliminarly look at what our variables ``Orders(thousands)'' andn ``Add-delete items(thousands)'' look like when graphed against the variable we hope to predict ``CPU time''}{section*.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{We can already see some patterns in the data but lets formalize this by performing a multiple linear regression to allow the company to predict their CPU Time with confidence based on the ``Orders(thousands)'' andn ``Add-delete items(thousands)''.}{4}{section*.13}\protected@file@percent }
\newlabel{we-can-already-see-some-patterns-in-the-data-but-lets-formalize-this-by-performing-a-multiple-linear-regression-to-allow-the-company-to-predict-their-cpu-time-with-confidence-based-on-the-ordersthousands-andn-add-delete-itemsthousands.}{{}{4}{We can already see some patterns in the data but lets formalize this by performing a multiple linear regression to allow the company to predict their CPU Time with confidence based on the ``Orders(thousands)'' andn ``Add-delete items(thousands)''}{section*.13}{}}
\@writefile{toc}{\contentsline {subsection}{DATA ANALYSIS: Multiple linear regression.}{4}{section*.14}\protected@file@percent }
\newlabel{data-analysis-multiple-linear-regression.}{{}{4}{DATA ANALYSIS: Multiple linear regression}{section*.14}{}}
\@writefile{toc}{\contentsline {subsubsection}{This is our final predictive function for the value of CPU time given the:}{4}{section*.15}\protected@file@percent }
\newlabel{this-is-our-final-predictive-function-for-the-value-of-cpu-time-given-the}{{}{4}{This is our final predictive function for the value of CPU time given the:}{section*.15}{}}
\@writefile{toc}{\contentsline {subsubsection}{And we get a \(r^2\) value of our linear prediction:}{4}{section*.16}\protected@file@percent }
\newlabel{and-we-get-a-r2-value-of-our-linear-prediction}{{}{4}{\texorpdfstring {And we get a \(r^2\) value of our linear prediction:}{And we get a r\^{}2 value of our linear prediction:}}{section*.16}{}}
\@writefile{toc}{\contentsline {subsubsection}{Next we find \(\mathaccentV {hat}05E{\sigma ^2}\) and \(\mathaccentV {hat}05E{Cov(\mathaccentV {vec}17E{\beta })}\)}{4}{section*.17}\protected@file@percent }
\newlabel{next-we-find-hatsigma2-and-hatcovvecbeta}{{}{4}{\texorpdfstring {Next we find \(\hat {\sigma ^2}\) and \(\hat {Cov(\vec {\beta })}\)}{Next we find \textbackslash {}hat\{\textbackslash {}sigma\^{}2\} and \textbackslash {}hat\{Cov(\textbackslash {}vec\{\textbackslash {}beta\})\}}}{section*.17}{}}
\@writefile{toc}{\contentsline {paragraph}{\(\mathaccentV {hat}05E{\sigma ^2}\)}{4}{section*.18}\protected@file@percent }
\newlabel{hatsigma2}{{}{4}{\texorpdfstring {\(\hat {\sigma ^2}\)}{\textbackslash {}hat\{\textbackslash {}sigma\^{}2\}}}{section*.18}{}}
\@writefile{toc}{\contentsline {paragraph}{\(\mathaccentV {hat}05E{Cov(\mathaccentV {vec}17E{\beta })}\)}{4}{section*.19}\protected@file@percent }
\newlabel{hatcovvecbeta}{{}{4}{\texorpdfstring {\(\hat {Cov(\vec {\beta })}\)}{\textbackslash {}hat\{Cov(\textbackslash {}vec\{\textbackslash {}beta\})\}}}{section*.19}{}}
\@writefile{toc}{\contentsline {subsubsection}{Find a 95\% confidence interval for the mean response \(E(Y_0|z_0)\) = \(\beta _0\) + \(\beta _1\)\(z_{01}\) + \(\beta _2\)\(z_{02}\) when \(z_0\)={[}1,130,7.5{]} (for example)}{4}{section*.20}\protected@file@percent }
\newlabel{find-a-95-confidence-interval-for-the-mean-response-ey_0z_0-beta_0-beta_1z_01-beta_2z_02-when-z_011307.5-for-example}{{}{4}{\texorpdfstring {Find a 95\% confidence interval for the mean response \(E(Y_0|z_0)\) = \(\beta _0\) + \(\beta _1\)\(z_{01}\) + \(\beta _2\)\(z_{02}\) when \(z_0\)={[}1,130,7.5{]} (for example)}{Find a 95\% confidence interval for the mean response E(Y\_0\textbar {}z\_0) = \textbackslash {}beta\_0 + \textbackslash {}beta\_1z\_\{01\} + \textbackslash {}beta\_2z\_\{02\} when z\_0={[}1,130,7.5{]} (for example)}}{section*.20}{}}
\@writefile{toc}{\contentsline {subsubsection}{Find a XX\% prediction interval for the mean response \(Y_0\) corresponding to \(\mathaccentV {bar}016{z_1}\), \(\mathaccentV {bar}016{z_2}\). This correspond to a 95\% prediction interval for a new facility's CPU requirement corresponding to the same \(z_0\).}{4}{section*.21}\protected@file@percent }
\newlabel{find-a-xx-prediction-interval-for-the-mean-response-y_0-corresponding-to-barz_1-barz_2.-this-correspond-to-a-95-prediction-interval-for-a-new-facilitys-cpu-requirement-corresponding-to-the-same-z_0.}{{}{4}{\texorpdfstring {Find a XX\% prediction interval for the mean response \(Y_0\) corresponding to \(\bar {z_1}\), \(\bar {z_2}\). This correspond to a 95\% prediction interval for a new facility's CPU requirement corresponding to the same \(z_0\).}{Find a XX\% prediction interval for the mean response Y\_0 corresponding to \textbackslash {}bar\{z\_1\}, \textbackslash {}bar\{z\_2\}. This correspond to a 95\% prediction interval for a new facility's CPU requirement corresponding to the same z\_0.}}{section*.21}{}}
\@writefile{toc}{\contentsline {subsection}{CONCLUSION}{4}{section*.22}\protected@file@percent }
\newlabel{conclusion}{{}{4}{CONCLUSION}{section*.22}{}}
\@writefile{toc}{\contentsline {subsubsection}{The company can use the following function to predict the amount of CPU time that they will need for a new set of parameters:}{4}{section*.23}\protected@file@percent }
\newlabel{the-company-can-use-the-following-function-to-predict-the-amount-of-cpu-time-that-they-will-need-for-a-new-set-of-parameters}{{}{4}{The company can use the following function to predict the amount of CPU time that they will need for a new set of parameters:}{section*.23}{}}
\@writefile{toc}{\contentsline {subsubsection}{Then, as generated in the last portion of the data analysis, we can see that if we want to be XX\% (where an alpha value of 0.05 corresponds to 95\%, alpha of 0.001 corresponds to 99.9\% etc.) sure that the true value of CPU time will fall within the lower and upper bound we will add and subtract the values generated to the value of y predicted above for some value of \(z_{01}\) being the orders in thousands and \(z_{02}\) being the add-delete items in thousands.}{5}{section*.24}\protected@file@percent }
\newlabel{then-as-generated-in-the-last-portion-of-the-data-analysis-we-can-see-that-if-we-want-to-be-xx-where-an-alpha-value-of-0.05-corresponds-to-95-alpha-of-0.001-corresponds-to-99.9-etc.-sure-that-the-true-value-of-cpu-time-will-fall-within-the-lower-and-upper-bound-we-will-add-and-subtract-the-values-generated-to-the-value-of-y-predicted-above-for-some-value-of-z_01-being-the-orders-in-thousands-and-z_02-being-the-add-delete-items-in-thousands.}{{}{5}{\texorpdfstring {Then, as generated in the last portion of the data analysis, we can see that if we want to be XX\% (where an alpha value of 0.05 corresponds to 95\%, alpha of 0.001 corresponds to 99.9\% etc.) sure that the true value of CPU time will fall within the lower and upper bound we will add and subtract the values generated to the value of y predicted above for some value of \(z_{01}\) being the orders in thousands and \(z_{02}\) being the add-delete items in thousands.}{Then, as generated in the last portion of the data analysis, we can see that if we want to be XX\% (where an alpha value of 0.05 corresponds to 95\%, alpha of 0.001 corresponds to 99.9\% etc.) sure that the true value of CPU time will fall within the lower and upper bound we will add and subtract the values generated to the value of y predicted above for some value of z\_\{01\} being the orders in thousands and z\_\{02\} being the add-delete items in thousands.}}{section*.24}{}}
\@writefile{toc}{\contentsline {paragraph}{In addtion, these values can automatically be generated with the following script by just altering the line z\_0 \textless {}- as.matrix(c(1,130,7.5)) by replacing "130 and 7.5 in the following manner. The general line would be z\_0 \textless {}- as.matrix(c(1,\(z_{01}\),\(z_{02}\))) where the value of \(z_{01}\) is the orders in thousands and \(z_{02}\) is the add-delete items in thousands.}{5}{section*.25}\protected@file@percent }
\newlabel{in-addtion-these-values-can-automatically-be-generated-with-the-following-script-by-just-altering-the-line-z_0---as.matrixc11307.5-by-replacing-130-and-7.5-in-the-following-manner.-the-general-line-would-be-z_0---as.matrixc1z_01z_02-where-the-value-of-z_01-is-the-orders-in-thousands-and-z_02-is-the-add-delete-items-in-thousands.}{{}{5}{\texorpdfstring {In addtion, these values can automatically be generated with the following script by just altering the line z\_0 \textless {}- as.matrix(c(1,130,7.5)) by replacing "130 and 7.5 in the following manner. The general line would be z\_0 \textless {}- as.matrix(c(1,\(z_{01}\),\(z_{02}\))) where the value of \(z_{01}\) is the orders in thousands and \(z_{02}\) is the add-delete items in thousands.}{In addtion, these values can automatically be generated with the following script by just altering the line z\_0 \textless {}- as.matrix(c(1,130,7.5)) by replacing "130 and 7.5 in the following manner. The general line would be z\_0 \textless {}- as.matrix(c(1,z\_\{01\},z\_\{02\})) where the value of z\_\{01\} is the orders in thousands and z\_\{02\} is the add-delete items in thousands.}}{section*.25}{}}
\@writefile{toc}{\contentsline {subsubsection}{As a precaution we do suggest gather more data on this situation if possible. This will increase the strength of the analysis and account for other variables in the situation. This would be achieved by increasing the value of n, or observed situaitons (company CPU time requirements), and r, the number of observed varaibles upon which to make the prediction of CPU time (orders, add-delete items, etc.). Please contact if this is the case and a more robust script will be made to factor in variations in new potential data gathering.}{5}{section*.26}\protected@file@percent }
\newlabel{as-a-precaution-we-do-suggest-gather-more-data-on-this-situation-if-possible.-this-will-increase-the-strength-of-the-analysis-and-account-for-other-variables-in-the-situation.-this-would-be-achieved-by-increasing-the-value-of-n-or-observed-situaitons-company-cpu-time-requirements-and-r-the-number-of-observed-varaibles-upon-which-to-make-the-prediction-of-cpu-time-orders-add-delete-items-etc..-please-contact-if-this-is-the-case-and-a-more-robust-script-will-be-made-to-factor-in-variations-in-new-potential-data-gathering.}{{}{5}{As a precaution we do suggest gather more data on this situation if possible. This will increase the strength of the analysis and account for other variables in the situation. This would be achieved by increasing the value of n, or observed situaitons (company CPU time requirements), and r, the number of observed varaibles upon which to make the prediction of CPU time (orders, add-delete items, etc.). Please contact if this is the case and a more robust script will be made to factor in variations in new potential data gathering}{section*.26}{}}
\@writefile{toc}{\contentsline {section}{DATASET 2: Two Sample Test and LDA}{6}{section*.27}\protected@file@percent }
\newlabel{dataset-2-two-sample-test-and-lda}{{}{6}{DATASET 2: Two Sample Test and LDA}{section*.27}{}}
\@writefile{toc}{\contentsline {subsection}{INTRODUCTION:}{6}{section*.28}\protected@file@percent }
\newlabel{introduction-1}{{}{6}{INTRODUCTION:}{section*.28}{}}
\@writefile{toc}{\contentsline {subsubsection}{Data has been gather on hemophilia A carriers.}{6}{section*.29}\protected@file@percent }
\newlabel{data-has-been-gather-on-hemophilia-a-carriers.}{{}{6}{Data has been gather on hemophilia A carriers}{section*.29}{}}
\@writefile{toc}{\contentsline {subsubsection}{The first goal of this analysis will be to perform two sample test on the data in order to see if there is a significant difference between individuals that are carriers and noncarriers with respect to AHF activity (\(log_{10}\) scale) and AHF antigen (\(log_{10}\) scale). Group 1 is considered to be the non carrier group while group 2 is considered to be the obligatory carrier group.}{6}{section*.30}\protected@file@percent }
\newlabel{the-first-goal-of-this-analysis-will-be-to-perform-two-sample-test-on-the-data-in-order-to-see-if-there-is-a-significant-difference-between-individuals-that-are-carriers-and-noncarriers-with-respect-to-ahf-activity-log_10-scale-and-ahf-antigen-log_10-scale.-group-1-is-considered-to-be-the-non-carrier-group-while-group-2-is-considered-to-be-the-obligatory-carrier-group.}{{}{6}{\texorpdfstring {The first goal of this analysis will be to perform two sample test on the data in order to see if there is a significant difference between individuals that are carriers and noncarriers with respect to AHF activity (\(log_{10}\) scale) and AHF antigen (\(log_{10}\) scale). Group 1 is considered to be the non carrier group while group 2 is considered to be the obligatory carrier group.}{The first goal of this analysis will be to perform two sample test on the data in order to see if there is a significant difference between individuals that are carriers and noncarriers with respect to AHF activity (log\_\{10\} scale) and AHF antigen (log\_\{10\} scale). Group 1 is considered to be the non carrier group while group 2 is considered to be the obligatory carrier group.}}{section*.30}{}}
\@writefile{toc}{\contentsline {subsubsection}{The second goal of this analysis will be to perform linear discriminant analysis to try and predict the best possible distinguishable boundary between carriers and non carriers with respect to AHF activity (\(log_{10}\) scale) and AHF antigen (\(log_{10}\) scale).}{6}{section*.31}\protected@file@percent }
\newlabel{the-second-goal-of-this-analysis-will-be-to-perform-linear-discriminant-analysis-to-try-and-predict-the-best-possible-distinguishable-boundary-between-carriers-and-non-carriers-with-respect-to-ahf-activity-log_10-scale-and-ahf-antigen-log_10-scale.}{{}{6}{\texorpdfstring {The second goal of this analysis will be to perform linear discriminant analysis to try and predict the best possible distinguishable boundary between carriers and non carriers with respect to AHF activity (\(log_{10}\) scale) and AHF antigen (\(log_{10}\) scale).}{The second goal of this analysis will be to perform linear discriminant analysis to try and predict the best possible distinguishable boundary between carriers and non carriers with respect to AHF activity (log\_\{10\} scale) and AHF antigen (log\_\{10\} scale).}}{section*.31}{}}
\@writefile{toc}{\contentsline {subsection}{SUMMARIZE DATA:}{6}{section*.32}\protected@file@percent }
\newlabel{summarize-data}{{}{6}{SUMMARIZE DATA:}{section*.32}{}}
\@writefile{toc}{\contentsline {subsubsection}{Lets look at the data as a whole (with the two groups together (carriers and non carriers))}{6}{section*.33}\protected@file@percent }
\newlabel{lets-look-at-the-data-as-a-whole-with-the-two-groups-together-carriers-and-non-carriers}{{}{6}{Lets look at the data as a whole (with the two groups together (carriers and non carriers))}{section*.33}{}}
\@writefile{toc}{\contentsline {subsection}{DATA ANALYSIS:}{8}{section*.34}\protected@file@percent }
\newlabel{data-analysis}{{}{8}{DATA ANALYSIS:}{section*.34}{}}
\@writefile{toc}{\contentsline {subsubsection}{Partial code credit to Prof Li (UC Davis), Weiping Zhang(USTC)}{8}{section*.35}\protected@file@percent }
\newlabel{partial-code-credit-to-prof-li-uc-davis-weiping-zhangustc}{{}{8}{Partial code credit to Prof Li (UC Davis), Weiping Zhang(USTC)}{section*.35}{}}
\@writefile{toc}{\contentsline {subsubsection}{Two sample test with:}{8}{section*.36}\protected@file@percent }
\newlabel{two-sample-test-with}{{}{8}{Two sample test with:}{section*.36}{}}
\@writefile{toc}{\contentsline {subsubsection}{Simultaneous confidence intervals}{8}{section*.37}\protected@file@percent }
\newlabel{simultaneous-confidence-intervals}{{}{8}{Simultaneous confidence intervals}{section*.37}{}}
\@writefile{toc}{\contentsline {paragraph}{We can now graph this confidence interval to better to see the confidence interval across the two axis to see which combinations of values would fall out of the confidence interval}{8}{section*.38}\protected@file@percent }
\newlabel{we-can-now-graph-this-confidence-interval-to-better-to-see-the-confidence-interval-across-the-two-axis-to-see-which-combinations-of-values-would-fall-out-of-the-confidence-interval}{{}{8}{We can now graph this confidence interval to better to see the confidence interval across the two axis to see which combinations of values would fall out of the confidence interval}{section*.38}{}}
\@writefile{toc}{\contentsline {subsubsection}{Bonferroni simultaneous confidence intervals}{9}{section*.39}\protected@file@percent }
\newlabel{bonferroni-simultaneous-confidence-intervals}{{}{9}{Bonferroni simultaneous confidence intervals}{section*.39}{}}
\@writefile{toc}{\contentsline {subsubsection}{Computing the LDA (linear discriminant analysis) this graphical representation will show us the boundary we will use as our cutoff when performing Fishers rule, based on the Mahalanobis distances.}{10}{section*.40}\protected@file@percent }
\newlabel{computing-the-lda-linear-discriminant-analysis-this-graphical-representation-will-show-us-the-boundary-we-will-use-as-our-cutoff-when-performing-fishers-rule-based-on-the-mahalanobis-distances.}{{}{10}{Computing the LDA (linear discriminant analysis) this graphical representation will show us the boundary we will use as our cutoff when performing Fishers rule, based on the Mahalanobis distances}{section*.40}{}}
\@writefile{toc}{\contentsline {subsubsection}{Some general setup for fishers rule. We will construct Fisher's rule. Moreover, we will calculate theapparent error rate. This is the number of incorrectly categorized variables based on the comparison of the Mahalanobis distances.}{10}{section*.41}\protected@file@percent }
\newlabel{some-general-setup-for-fishers-rule.-we-will-construct-fishers-rule.-moreover-we-will-calculate-theapparent-error-rate.-this-is-the-number-of-incorrectly-categorized-variables-based-on-the-comparison-of-the-mahalanobis-distances.}{{}{10}{Some general setup for fishers rule. We will construct Fisher's rule. Moreover, we will calculate theapparent error rate. This is the number of incorrectly categorized variables based on the comparison of the Mahalanobis distances}{section*.41}{}}
\@writefile{toc}{\contentsline {paragraph}{Error rate for Apparent error rate:}{10}{section*.42}\protected@file@percent }
\newlabel{error-rate-for-apparent-error-rate}{{}{10}{Error rate for Apparent error rate:}{section*.42}{}}
\@writefile{toc}{\contentsline {paragraph}{Number incorrect non-carrier:}{10}{section*.43}\protected@file@percent }
\newlabel{number-incorrect-non-carrier}{{}{10}{Number incorrect non-carrier:}{section*.43}{}}
\@writefile{toc}{\contentsline {paragraph}{Number incorrect carrier:}{10}{section*.44}\protected@file@percent }
\newlabel{number-incorrect-carrier}{{}{10}{Number incorrect carrier:}{section*.44}{}}
\@writefile{toc}{\contentsline {subsubsection}{Lets overlay the ones that were incorrect for each group as a different color on our LDA graph just to confirm. This will allow us to compare where this error rate model compares to Lachenbruch's holdout.}{11}{section*.45}\protected@file@percent }
\newlabel{lets-overlay-the-ones-that-were-incorrect-for-each-group-as-a-different-color-on-our-lda-graph-just-to-confirm.-this-will-allow-us-to-compare-where-this-error-rate-model-compares-to-lachenbruchs-holdout.}{{}{11}{Lets overlay the ones that were incorrect for each group as a different color on our LDA graph just to confirm. This will allow us to compare where this error rate model compares to Lachenbruch's holdout}{section*.45}{}}
\@writefile{toc}{\contentsline {subsubsection}{Finally the expected error rate by Lachenbruch's holdout: this will help fix the fact that apparent error rate can often underestimate the error rate by witholding individual entries, observations in the data and then performing a LDA or Mahalanobis distance without this entry. This entry is then tested in the model as a new observation to see if it is correctly classified or not. As we can see from the graphical representation this can change our results around the border of the LDA.}{12}{section*.46}\protected@file@percent }
\newlabel{finally-the-expected-error-rate-by-lachenbruchs-holdout-this-will-help-fix-the-fact-that-apparent-error-rate-can-often-underestimate-the-error-rate-by-witholding-individual-entries-observations-in-the-data-and-then-performing-a-lda-or-mahalanobis-distance-without-this-entry.-this-entry-is-then-tested-in-the-model-as-a-new-observation-to-see-if-it-is-correctly-classified-or-not.-as-we-can-see-from-the-graphical-representation-this-can-change-our-results-around-the-border-of-the-lda.}{{}{12}{Finally the expected error rate by Lachenbruch's holdout: this will help fix the fact that apparent error rate can often underestimate the error rate by witholding individual entries, observations in the data and then performing a LDA or Mahalanobis distance without this entry. This entry is then tested in the model as a new observation to see if it is correctly classified or not. As we can see from the graphical representation this can change our results around the border of the LDA}{section*.46}{}}
\@writefile{toc}{\contentsline {paragraph}{Error rate for Apparent error rate:}{12}{section*.47}\protected@file@percent }
\newlabel{error-rate-for-apparent-error-rate-1}{{}{12}{Error rate for Apparent error rate:}{section*.47}{}}
\@writefile{toc}{\contentsline {paragraph}{Number incorrect non-carrier:}{12}{section*.48}\protected@file@percent }
\newlabel{number-incorrect-non-carrier-1}{{}{12}{Number incorrect non-carrier:}{section*.48}{}}
\@writefile{toc}{\contentsline {paragraph}{Number incorrect carrier:}{12}{section*.49}\protected@file@percent }
\newlabel{number-incorrect-carrier-1}{{}{12}{Number incorrect carrier:}{section*.49}{}}
\@writefile{toc}{\contentsline {paragraph}{Lets overlay the ones that were incorrect for each group as a different color on our LDA graph just to confirm our numbers and visualize where Lachenbruch's holdout is changing the error rate in comparison to the APR.}{12}{section*.50}\protected@file@percent }
\newlabel{lets-overlay-the-ones-that-were-incorrect-for-each-group-as-a-different-color-on-our-lda-graph-just-to-confirm-our-numbers-and-visualize-where-lachenbruchs-holdout-is-changing-the-error-rate-in-comparison-to-the-apr.}{{}{12}{Lets overlay the ones that were incorrect for each group as a different color on our LDA graph just to confirm our numbers and visualize where Lachenbruch's holdout is changing the error rate in comparison to the APR}{section*.50}{}}
\@writefile{toc}{\contentsline {subsection}{CONCLUSION}{14}{section*.51}\protected@file@percent }
\newlabel{conclusion-1}{{}{14}{CONCLUSION}{section*.51}{}}
\@writefile{toc}{\contentsline {subsubsection}{As a precaution we do suggest gather more data on this situation if possible to create a more robust model. A common method in machine learning, at least from personal experience, for assessing the error rate of a model is to build a model on a proportion of the data. This is different from the Lachenbruch's holdout in the fact that we may only use 50-70\% of the data and then test the error rate of the model on the remaining 30-50\%. Obviously this proportions can greatly vary but they require that a lot more data is collected.}{14}{section*.52}\protected@file@percent }
\newlabel{as-a-precaution-we-do-suggest-gather-more-data-on-this-situation-if-possible-to-create-a-more-robust-model.-a-common-method-in-machine-learning-at-least-from-personal-experience-for-assessing-the-error-rate-of-a-model-is-to-build-a-model-on-a-proportion-of-the-data.-this-is-different-from-the-lachenbruchs-holdout-in-the-fact-that-we-may-only-use-50-70-of-the-data-and-then-test-the-error-rate-of-the-model-on-the-remaining-30-50.-obviously-this-proportions-can-greatly-vary-but-they-require-that-a-lot-more-data-is-collected.}{{}{14}{As a precaution we do suggest gather more data on this situation if possible to create a more robust model. A common method in machine learning, at least from personal experience, for assessing the error rate of a model is to build a model on a proportion of the data. This is different from the Lachenbruch's holdout in the fact that we may only use 50-70\% of the data and then test the error rate of the model on the remaining 30-50\%. Obviously this proportions can greatly vary but they require that a lot more data is collected}{section*.52}{}}
\@writefile{toc}{\contentsline {subsubsection}{The first goal of this analysis was to perform two sample test on the data in order to see if there is a significant difference between individuals that are carriers and noncarriers with respect to AHF activity (\(log_{10}\) scale) and AHF antigen (\(log_{10}\) scale). Group 1 is considered to be the non carrier group while group 2 is considered to be the obligatory carrier group. Using the Hotelling's \(T^2\) test we see that at the 95\% confidence intervale we we reject the null hypothesis, that the two samples are from the same populations with the same multivariate means.}{14}{section*.53}\protected@file@percent }
\newlabel{the-first-goal-of-this-analysis-was-to-perform-two-sample-test-on-the-data-in-order-to-see-if-there-is-a-significant-difference-between-individuals-that-are-carriers-and-noncarriers-with-respect-to-ahf-activity-log_10-scale-and-ahf-antigen-log_10-scale.-group-1-is-considered-to-be-the-non-carrier-group-while-group-2-is-considered-to-be-the-obligatory-carrier-group.-using-the-hotellings-t2-test-we-see-that-at-the-95-confidence-intervale-we-we-reject-the-null-hypothesis-that-the-two-samples-are-from-the-same-populations-with-the-same-multivariate-means.}{{}{14}{\texorpdfstring {The first goal of this analysis was to perform two sample test on the data in order to see if there is a significant difference between individuals that are carriers and noncarriers with respect to AHF activity (\(log_{10}\) scale) and AHF antigen (\(log_{10}\) scale). Group 1 is considered to be the non carrier group while group 2 is considered to be the obligatory carrier group. Using the Hotelling's \(T^2\) test we see that at the 95\% confidence intervale we we reject the null hypothesis, that the two samples are from the same populations with the same multivariate means.}{The first goal of this analysis was to perform two sample test on the data in order to see if there is a significant difference between individuals that are carriers and noncarriers with respect to AHF activity (log\_\{10\} scale) and AHF antigen (log\_\{10\} scale). Group 1 is considered to be the non carrier group while group 2 is considered to be the obligatory carrier group. Using the Hotelling's T\^{}2 test we see that at the 95\% confidence intervale we we reject the null hypothesis, that the two samples are from the same populations with the same multivariate means.}}{section*.53}{}}
\@writefile{toc}{\contentsline {subsubsection}{The second goal of this analysis will be to perform linear discriminant analysis to try and predict the best possible distinguishable boundary between carriers and non carriers with respect to AHF activity (\(log_{10}\) scale) and AHF antigen (\(log_{10}\) scale). By performing Fisher's rule we were able to obtain an error rate of 0.16 for Lachenbruch's holdout. Overall the results from the LDA will allow physicians to diagnose someone with better confidence, especially if used in conjuction with other diagnostic tests. These sorts of tests are often used with Bayes to estimate the probability someone has the condition, given the prevalence in the population itself. This will increase certainty of classications, but as mentioned earlier it is always highly suggested to collect more data from a larger variety of circumstances as this data collected could not be representative of the population.}{14}{section*.54}\protected@file@percent }
\newlabel{the-second-goal-of-this-analysis-will-be-to-perform-linear-discriminant-analysis-to-try-and-predict-the-best-possible-distinguishable-boundary-between-carriers-and-non-carriers-with-respect-to-ahf-activity-log_10-scale-and-ahf-antigen-log_10-scale.-by-performing-fishers-rule-we-were-able-to-obtain-an-error-rate-of-0.16-for-lachenbruchs-holdout.-overall-the-results-from-the-lda-will-allow-physicians-to-diagnose-someone-with-better-confidence-especially-if-used-in-conjuction-with-other-diagnostic-tests.-these-sorts-of-tests-are-often-used-with-bayes-to-estimate-the-probability-someone-has-the-condition-given-the-prevalence-in-the-population-itself.-this-will-increase-certainty-of-classications-but-as-mentioned-earlier-it-is-always-highly-suggested-to-collect-more-data-from-a-larger-variety-of-circumstances-as-this-data-collected-could-not-be-representative-of-the-population.}{{}{14}{\texorpdfstring {The second goal of this analysis will be to perform linear discriminant analysis to try and predict the best possible distinguishable boundary between carriers and non carriers with respect to AHF activity (\(log_{10}\) scale) and AHF antigen (\(log_{10}\) scale). By performing Fisher's rule we were able to obtain an error rate of 0.16 for Lachenbruch's holdout. Overall the results from the LDA will allow physicians to diagnose someone with better confidence, especially if used in conjuction with other diagnostic tests. These sorts of tests are often used with Bayes to estimate the probability someone has the condition, given the prevalence in the population itself. This will increase certainty of classications, but as mentioned earlier it is always highly suggested to collect more data from a larger variety of circumstances as this data collected could not be representative of the population.}{The second goal of this analysis will be to perform linear discriminant analysis to try and predict the best possible distinguishable boundary between carriers and non carriers with respect to AHF activity (log\_\{10\} scale) and AHF antigen (log\_\{10\} scale). By performing Fisher's rule we were able to obtain an error rate of 0.16 for Lachenbruch's holdout. Overall the results from the LDA will allow physicians to diagnose someone with better confidence, especially if used in conjuction with other diagnostic tests. These sorts of tests are often used with Bayes to estimate the probability someone has the condition, given the prevalence in the population itself. This will increase certainty of classications, but as mentioned earlier it is always highly suggested to collect more data from a larger variety of circumstances as this data collected could not be representative of the population.}}{section*.54}{}}
\@writefile{toc}{\contentsline {section}{DATASET 3: PCA}{15}{section*.55}\protected@file@percent }
\newlabel{dataset-3-pca}{{}{15}{DATASET 3: PCA}{section*.55}{}}
\@writefile{toc}{\contentsline {subsection}{INTRODUCTION:}{15}{section*.56}\protected@file@percent }
\newlabel{introduction-2}{{}{15}{INTRODUCTION:}{section*.56}{}}
\@writefile{toc}{\contentsline {subsubsection}{Data has been gatherd on populations which is considered ``Census-tract data''. The variables that have been gathered are ``Total Population (thousands)'', ``Professional Degree (\%)'', ``Employed age over 16 (\%)'', ``Government Employment (\%)'', ``Median home value (\$100,000s)''.}{15}{section*.57}\protected@file@percent }
\newlabel{data-has-been-gatherd-on-populations-which-is-considered-census-tract-data.-the-variables-that-have-been-gathered-are-total-population-thousands-professional-degree-employed-age-over-16-government-employment-median-home-value-100000s.}{{}{15}{Data has been gatherd on populations which is considered ``Census-tract data''. The variables that have been gathered are ``Total Population (thousands)'', ``Professional Degree (\%)'', ``Employed age over 16 (\%)'', ``Government Employment (\%)'', ``Median home value (\$100,000s)''}{section*.57}{}}
\@writefile{toc}{\contentsline {subsubsection}{PCA (principle component analysis) is a popular way to explore datasets with multiple dimensions due to the fact that it is a diminsional reduction technique which allows exploration of the data in 2 dimensions (depending on how much of the variance can be summarized in those two dimensions). This is a great high level method to explore which groups have the most variation and potentially cluster together; therefore, this data can help those interested, such as politicians, in understanding their demographic.}{15}{section*.58}\protected@file@percent }
\newlabel{pca-principle-component-analysis-is-a-popular-way-to-explore-datasets-with-multiple-dimensions-due-to-the-fact-that-it-is-a-diminsional-reduction-technique-which-allows-exploration-of-the-data-in-2-dimensions-depending-on-how-much-of-the-variance-can-be-summarized-in-those-two-dimensions.-this-is-a-great-high-level-method-to-explore-which-groups-have-the-most-variation-and-potentially-cluster-together-therefore-this-data-can-help-those-interested-such-as-politicians-in-understanding-their-demographic.}{{}{15}{PCA (principle component analysis) is a popular way to explore datasets with multiple dimensions due to the fact that it is a diminsional reduction technique which allows exploration of the data in 2 dimensions (depending on how much of the variance can be summarized in those two dimensions). This is a great high level method to explore which groups have the most variation and potentially cluster together; therefore, this data can help those interested, such as politicians, in understanding their demographic}{section*.58}{}}
\@writefile{toc}{\contentsline {subsection}{SUMMARIZE DATA:}{15}{section*.59}\protected@file@percent }
\newlabel{summarize-data-1}{{}{15}{SUMMARIZE DATA:}{section*.59}{}}
\@writefile{toc}{\contentsline {subsubsection}{Lets summarize the dataframe we are using for the problem, or the census variates for a set of observations.}{15}{section*.60}\protected@file@percent }
\newlabel{lets-summarize-the-dataframe-we-are-using-for-the-problem-or-the-census-variates-for-a-set-of-observations.}{{}{15}{Lets summarize the dataframe we are using for the problem, or the census variates for a set of observations}{section*.60}{}}
\@writefile{toc}{\contentsline {subsection}{DATA ANALYSIS:}{16}{section*.61}\protected@file@percent }
\newlabel{data-analysis-1}{{}{16}{DATA ANALYSIS:}{section*.61}{}}
\@writefile{toc}{\contentsline {subsubsection}{First lets take a look at the eigen values and eigen vectors of the covariance matrix as these are important factors in the principle component analysis. In addtion, we can look at the correlation coeffecients between our principle components and our variates, which is another method of analyzing the contribution of each variate.}{16}{section*.62}\protected@file@percent }
\newlabel{first-lets-take-a-look-at-the-eigen-values-and-eigen-vectors-of-the-covariance-matrix-as-these-are-important-factors-in-the-principle-component-analysis.-in-addtion-we-can-look-at-the-correlation-coeffecients-between-our-principle-components-and-our-variates-which-is-another-method-of-analyzing-the-contribution-of-each-variate.}{{}{16}{First lets take a look at the eigen values and eigen vectors of the covariance matrix as these are important factors in the principle component analysis. In addtion, we can look at the correlation coeffecients between our principle components and our variates, which is another method of analyzing the contribution of each variate}{section*.62}{}}
\@writefile{toc}{\contentsline {subsection}{Also, lets take a look at the eigen values and eigen vectors of the standardized matrix which is also sometimes a good way to summarize our data (can be preferrable if the primary eigen values are a larger proportion than that of the covariance matrix). As expected these values are the same since the loadings of the standardized matrix are the same as the correlations coeffecients of the standardized matrix.}{16}{section*.63}\protected@file@percent }
\newlabel{also-lets-take-a-look-at-the-eigen-values-and-eigen-vectors-of-the-standardized-matrix-which-is-also-sometimes-a-good-way-to-summarize-our-data-can-be-preferrable-if-the-primary-eigen-values-are-a-larger-proportion-than-that-of-the-covariance-matrix.-as-expected-these-values-are-the-same-since-the-loadings-of-the-standardized-matrix-are-the-same-as-the-correlations-coeffecients-of-the-standardized-matrix.}{{}{16}{Also, lets take a look at the eigen values and eigen vectors of the standardized matrix which is also sometimes a good way to summarize our data (can be preferrable if the primary eigen values are a larger proportion than that of the covariance matrix). As expected these values are the same since the loadings of the standardized matrix are the same as the correlations coeffecients of the standardized matrix}{section*.63}{}}
\@writefile{toc}{\contentsline {subsubsection}{Lets see if the covariance matrix or the correlation matrix summarizes the data in the first two components better.}{17}{section*.64}\protected@file@percent }
\newlabel{lets-see-if-the-covariance-matrix-or-the-correlation-matrix-summarizes-the-data-in-the-first-two-components-better.}{{}{17}{Lets see if the covariance matrix or the correlation matrix summarizes the data in the first two components better}{section*.64}{}}
\@writefile{toc}{\contentsline {paragraph}{Variance summarized in first two components of the covariance matrix:}{17}{section*.65}\protected@file@percent }
\newlabel{variance-summarized-in-first-two-components-of-the-covariance-matrix}{{}{17}{Variance summarized in first two components of the covariance matrix:}{section*.65}{}}
\@writefile{toc}{\contentsline {paragraph}{Variance summarized in first two components of the covariance of standardized matrix:}{17}{section*.66}\protected@file@percent }
\newlabel{variance-summarized-in-first-two-components-of-the-covariance-of-standardized-matrix}{{}{17}{Variance summarized in first two components of the covariance of standardized matrix:}{section*.66}{}}
\@writefile{toc}{\contentsline {paragraph}{So we see that using the covariance matrix is preferred here.}{17}{section*.67}\protected@file@percent }
\newlabel{so-we-see-that-using-the-covariance-matrix-is-preferred-here.}{{}{17}{So we see that using the covariance matrix is preferred here}{section*.67}{}}
\@writefile{toc}{\contentsline {subsubsection}{So since we can summarize \textasciitilde {}93\% of the variance in our data using the first two principal components it would fair to graph our analysis using these two PCs. Lets also looks the the eigen value size graphed over the five components. Finally, we will overlay the PC scores for the sample data in the space of the first two principal components so we can visualize which of the sample data is most contributing to the the principal components which we see Government employment percentage and employed age over 16 (\%) are the main two contributors to the}{17}{section*.68}\protected@file@percent }
\newlabel{so-since-we-can-summarize-93-of-the-variance-in-our-data-using-the-first-two-principal-components-it-would-fair-to-graph-our-analysis-using-these-two-pcs.-lets-also-looks-the-the-eigen-value-size-graphed-over-the-five-components.-finally-we-will-overlay-the-pc-scores-for-the-sample-data-in-the-space-of-the-first-two-principal-components-so-we-can-visualize-which-of-the-sample-data-is-most-contributing-to-the-the-principal-components-which-we-see-government-employment-percentage-and-employed-age-over-16-are-the-main-two-contributors-to-the}{{}{17}{So since we can summarize \textasciitilde {}93\% of the variance in our data using the first two principal components it would fair to graph our analysis using these two PCs. Lets also looks the the eigen value size graphed over the five components. Finally, we will overlay the PC scores for the sample data in the space of the first two principal components so we can visualize which of the sample data is most contributing to the the principal components which we see Government employment percentage and employed age over 16 (\%) are the main two contributors to the}{section*.68}{}}
\@writefile{toc}{\contentsline {subsubsection}{Now the scaled or standarized dataset}{19}{section*.69}\protected@file@percent }
\newlabel{now-the-scaled-or-standarized-dataset}{{}{19}{Now the scaled or standarized dataset}{section*.69}{}}
\@writefile{toc}{\contentsline {subsection}{CONCLUSION}{20}{section*.70}\protected@file@percent }
\newlabel{conclusion-2}{{}{20}{CONCLUSION}{section*.70}{}}
\@writefile{toc}{\contentsline {subsubsection}{As a precaution we do suggest gather more data on this situation if possible. This will increase the strength of the analysis and account for other variables in the situation. This would be achieved by increasing the value of n, or observed situaitons (census data in new areas), and r, the number of observed varaibles upon which to build PCs (such as ``Total\_Population(thousands)'', ``Professional\_Degree(\%)'', and ``Employed\_age\_over\_16(\%)'' etc). Please contact if this is the case and a more robust script will be made to factor in variations in new potential data gathering.}{20}{section*.71}\protected@file@percent }
\newlabel{as-a-precaution-we-do-suggest-gather-more-data-on-this-situation-if-possible.-this-will-increase-the-strength-of-the-analysis-and-account-for-other-variables-in-the-situation.-this-would-be-achieved-by-increasing-the-value-of-n-or-observed-situaitons-census-data-in-new-areas-and-r-the-number-of-observed-varaibles-upon-which-to-build-pcs-such-as-total_populationthousands-professional_degree-and-employed_age_over_16-etc.-please-contact-if-this-is-the-case-and-a-more-robust-script-will-be-made-to-factor-in-variations-in-new-potential-data-gathering.}{{}{20}{As a precaution we do suggest gather more data on this situation if possible. This will increase the strength of the analysis and account for other variables in the situation. This would be achieved by increasing the value of n, or observed situaitons (census data in new areas), and r, the number of observed varaibles upon which to build PCs (such as ``Total\_Population(thousands)'', ``Professional\_Degree(\%)'', and ``Employed\_age\_over\_16(\%)'' etc). Please contact if this is the case and a more robust script will be made to factor in variations in new potential data gathering}{section*.71}{}}
\@writefile{toc}{\contentsline {subsubsection}{From the results of the principle component analysis we are able to determine that we can summarize \%92 of the variance in our dataset in the first princple component. We also are able to determine that two variates that contribute most to the variance in the population are Government Employment and Age over 16 employement rates. This type of information may be of value to a variety of people who may try to better understand the demographic. Individuals can better influence these regions based on things they have in common (home value, total population, professional degrees etc.), but they can also target other sectors more specifically based on government employment and individuals employed over the age of 16.}{20}{section*.72}\protected@file@percent }
\newlabel{from-the-results-of-the-principle-component-analysis-we-are-able-to-determine-that-we-can-summarize-92-of-the-variance-in-our-dataset-in-the-first-princple-component.-we-also-are-able-to-determine-that-two-variates-that-contribute-most-to-the-variance-in-the-population-are-government-employment-and-age-over-16-employement-rates.-this-type-of-information-may-be-of-value-to-a-variety-of-people-who-may-try-to-better-understand-the-demographic.-individuals-can-better-influence-these-regions-based-on-things-they-have-in-common-home-value-total-population-professional-degrees-etc.-but-they-can-also-target-other-sectors-more-specifically-based-on-government-employment-and-individuals-employed-over-the-age-of-16.}{{}{20}{From the results of the principle component analysis we are able to determine that we can summarize \%92 of the variance in our dataset in the first princple component. We also are able to determine that two variates that contribute most to the variance in the population are Government Employment and Age over 16 employement rates. This type of information may be of value to a variety of people who may try to better understand the demographic. Individuals can better influence these regions based on things they have in common (home value, total population, professional degrees etc.), but they can also target other sectors more specifically based on government employment and individuals employed over the age of 16}{section*.72}{}}
