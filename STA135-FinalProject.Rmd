---
title: "Final Project"
author: "Keith Mitchell"
date: "3/7/2020"
output:
  pdf_document: default
  html_document: default
---

https://www.chegg.com/homework-help/Applied-Multivariate-Statistical-Analysis-6th-edition-chapter-11-problem-32E-solution-9780131877153
https://www.chegg.com/homework-help/Applied-Multivariate-Statistical-Analysis-6th-edition-chapter-8-problem-11E-solution-9780131877153

\begin{center}
STA135 Final Project (UC Davis)
\end{center}

\begin{center}
Instructor: Professor Li
\end{center}

\begin{center}
TA: Cong Xu
\end{center}

---

## Dataset 1: Conduct multiple linear regression;
## Dataset 2: Conduct two-sample test and LDA;
## Dataset 3: Conduct PCA.

### For each data analysis, you should write in full sentences, and have thefollowing sections for the body of your report.
1) Introduction: Briefly summarize the goal of the analysis in your own words;
2) Summary: Summarize your data by plots or sample estimates; 
3) Analysis: Implement the analysis based on what you have done in homework;
4) Conclusion: Describe and interpret your findings.

### Details:
1) A title page including your name, the name of the class, and the name of your instructor.
2) Do not include R code in the body of your report.  R code used to produce the results should all go to the appendix.(echo=FALSE)
3) Typed.
4) Double-sided pages.


---

<P style="page-break-before: always">
\newpage


---
# DATASET 1: Multiple Linear Regression


## INTRODUCTION: 
### A company is interested in considering the purchase of a computer but first must assess their future needs in order to determine the proper equipment. A computer scientist collected data from seven similar company sites so that a forecast equation of computer-hardware requirements for inventory management could be developed. (pg 380) This is an important concept for companies running web infrastructure to predict for a variety of reasons. 
1) It is preferable not to overpay for CPU time that will likely not get used for the company web platform. 
2) It is important for the stability of the web platform that it has the necessary CPU resources to maintain the stability of their web platform to keep customers happy and making purchases. 
### The goal of this analysis will be to introduce a model that can best predict the necessary CPU time needed given a set of their own variables ("Orders(thousands)" and "Add-delete items(thousands)") iwith a high degree of confidence. 




## SUMMARIZE DATA: The first colum of our data is the Orders(in thousands), and the Second column is the Add-Delete items (in thousands)
### Lets summarize our data first and see what the numbers look like. 
```{r echo=FALSE}
data <- read.table("T7-3.DAT", 
           header=FALSE)
colnames(data) <- c("Orders(thousands)", "Add-delete items(thousands)")
Z <- as.matrix(cbind(1, data[1], data[2]))
y <- as.matrix(data[3])
colnames(y) <- c("CPU time(hours)")
Z
summary(Z)
y
summary(y)
```
### Now lets visualize these numbers with a box plot of each of the variables, including the one we are trying to predict.
```{r echo=FALSE}
boxplot(y, ylab="CPU time")
boxplot(Z[,2], ylab="Orders(thousands)")
boxplot(Z[,3], ylab="Add-delete items(thousands)")
```

### Lets also just get a preliminarly look at what our variables "Orders(thousands)" andn "Add-delete items(thousands)" look like when graphed against the variable we hope to predict "CPU time"
```{r echo=FALSE}
plot(Z[,2],y, ylab="CPU time", xlab="Orders(thousands)")
plot(Z[,3],y, ylab="CPU time", xlab="Add-delete items(thousands)")
```

### We can already see some patterns in the data but lets formalize this by performing a multiple linear regression to allow the company to predict their CPU Time with confidence based on the "Orders(thousands)" andn "Add-delete items(thousands)". 


## DATA ANALYSIS: Multiple linear regression.
### This is our final predictive function for the value of CPU time given the:
```{r echo=FALSE}
beta_hat <- solve(t(Z)%*%Z)%*%t(Z)%*%y
y_hat <- Z%*%beta_hat
print(sprintf("y = %s*z_1 + %s*z_2 + %s", beta_hat[2], beta_hat[3], beta_hat[1]))
```

### And we get a \(r^2\) value of:
```{r echo=FALSE}

ess <- (y_hat-mean(y))^2
tss <- (y-mean(y))^2
r_sq <- sum(ess)/sum(tss)
r_sq
```


### Next we find \(\hat{\sigma^2}\) and \(\hat{Cov(\vec{\beta})}\)
- n is the number of observed instances (or data to learn from) for each r.
- This is equal to the number of rows in Z = 7
- r is the number of observed variables for which each n has a value. 
- So this is number of columns of data in Z = 2
```{r echo=FALSE}
n <- 7
r <- 2
y_hat <- Z%*%beta_hat
eps_hat <- y-y_hat
sigma_sq <- (1/(n-r-1))*(sum(eps_hat^2))
sigma_sq
```

```{r echo=FALSE}
cov_hat_of_beta <- sigma_sq*solve(t(Z)%*%Z)
cov_hat_of_beta
```


### Find a 95% confidence interval for the mean response \(E(Y_0|z_0)\) =  \(\beta_0\) + \(\beta_1\)\(z_{01}\) + \(\beta_2\)\(z_{02}\) when \(z_0\)=[1,130,7.5] (for example)
```{r echo=FALSE}
# here we can predict the value of y given any values of x of interest that the company may have
z_0 <- as.matrix(c(1,130,7.5))
z_0_beta_hat <- t(z_0)%*%beta_hat
statistic<-qt(1-(0.05/2),n-r-1)
ans<-t(z_0)%*%(solve(t(Z)%*%Z))%*%z_0
interval <- sqrt(sigma_sq)*statistic*sqrt(ans)
int_low <- z_0_beta_hat - interval
int_up <- z_0_beta_hat + interval
int_low
int_up
```

### Find a XX% prediction interval for the mean response \(Y_0\) corresponding to \(\bar{z_1}\), \(\bar{z_2}\). This correspond to a 95% prediction interval for a new facility's CPU requirement corresponding to the same \(z_0\).
```{r echo=FALSE}

for (val in c(0.05,0.01,0.005,0.001))
{
  z_0 <- as.matrix(c(1,130,7.5))
  z_0_beta_hat <- t(z_0)%*%beta_hat
  statistic<-qt(1-(val/2),n-r-1)
  ans<-t(z_0)%*%(solve(t(Z)%*%Z))%*%z_0
  interval <- sqrt(sigma_sq)*statistic*sqrt(1+ans)
  int_low <- z_0_beta_hat - interval
  int_up <- z_0_beta_hat + interval
  print (sprintf("Alpha value: %s  Interval: %s  Lower bound: %s  Upper bound: %s",  val, interval, int_low, int_up ))
}
```


## CONCLUSION
### The company can use the following function to predict the amount of CPU time that they will need for a new set of parameters: 
- y = 1.07898250130454\(z_1\) + 0.419888473166963\(z_2\) + 8.42368896741667
  - \(z_1\)  is the value of the companies number of orders (in thousands) that they need to know CPU time requirements for
  - \(z_2\)  is the companies add-delete items (in thousands) that they need to know CPU time requirements for
  - y is the predicted CPU time needed. 

### Then, as generated in the last portion of the data analysis, we can see that if we want to be XX% (where an alpha value of 0.05 corresponds to 95%, alpha of 0.001 corresponds to 99.9% etc.) sure that the true value of CPU time will fall within the lower and upper bound we will add and subtract the values generated to the value of y predicted above for some value of \(z_{01}\) being the orders in thousands and \(z_{02}\) being the add-delete items in thousands. 
- Alpha value: 0.05  Interval: 3.91241721099891
- Alpha value: 0.01  Interval: 6.48784302704889
- Alpha value: 0.005  Interval: 7.88779247898333
- Alpha value: 0.001  Interval: 12.1331741930768

#### In addtion, these values can automatically be generated with the following script by just altering the line z_0 <- as.matrix(c(1,130,7.5)) by replacing "130 and 7.5 in the following manner. The general line would be z_0 <- as.matrix(c(1,\(z_{01}\),\(z_{02}\))) where the value of \(z_{01}\) is the orders in thousands and \(z_{02}\) is the add-delete items in thousands. 
```
for (val in c(0.05,0.01,0.005,0.001))
{
  z_0 <- as.matrix(c(1,130,7.5))
  z_0_beta_hat <- t(z_0)%*%beta_hat
  statistic<-qt(1-(val/2),n-r-1)
  ans<-t(z_0)%*%(solve(t(Z)%*%Z))%*%z_0
  interval <- sqrt(sigma_sq)*statistic*sqrt(1+ans)
  int_low <- z_0_beta_hat - interval
  int_up <- z_0_beta_hat + interval
  print (sprintf("Alpha value: %s  Interval: %s  Lower bound: %s  Upper bound: %s",  val, interval, int_low, int_up ))
}
```


### As a precaution we do suggest gather more data on this situation if possible. This will increase the strength of the analysis and account for other variables in the situation. This would be achieved by increasing the value of n, or observed situaitons (company CPU time requirements), and r, the number of observed varaibles upon which to make the prediction of CPU time (orders, add-delete items, etc.). Please contact if this is the case and a more robust script will be made to factor in variations in new potential data gathering.


#TODO: what is the big difference of adding the +1 in the
#TODO: can i create covariance matrix the way i did here ... same question for hw6


---
<P style="page-break-before: always">
\newpage






---

# DATASET 2: Two Sample Test and LDA


## INTRODUCTION: 
### Data has been gather on hemophilia A carriers. 
1) Predicting disease based on other characteristics is a common technique that doctors and healthcare workers use to produce a prediction for a given person. 
2) This is a common method that if not used solely for diagnosing a disease is used in conjunction with other tests to increase the statistical power of the diagnoses.
### The first goal of this analysis will be to perform two sample test on the data in order to see if there is a significant difference between individuals that are carriers and noncarriers with respect to AHF activity (\(log_{10}\) scale) and AHF antigen (\(log_{10}\) scale). Group 1 is considered to be the non carrier group while group 2 is considered to be the obligatory carrier group. 
### The second goal of this analysis will be to perform linear discriminant analysis to try and predict the best possible distinguishable boundary between carriers and non carriers with respect to AHF activity (\(log_{10}\) scale) and AHF antigen (\(log_{10}\) scale).


## SUMMARIZE DATA:
### Lets look at the data as a whole (with the two groups together (carriers and non carriers))
```{r echo=FALSE}
data <- read.table("T11-8.DAT", 
           header=FALSE, colClasses = c("factor", "numeric", "numeric"))
colnames(data) <- c("Group", "AHF_activity", "AHF_antigen")
levels(data$Group)[levels(data$Group)=="1"] <- "Non-carrier"
levels(data$Group)[levels(data$Group)=="2"] <- "Carrier"
X <- as.matrix(cbind(data[2], data[3]))
dim(X)
summary(data)
```


```{r echo=FALSE}
library(ggplot2)
ggplot(data, aes(x=AHF_activity, y=AHF_antigen, color=Group)) + geom_point()

```


```{r echo=FALSE}
boxplot(X[,1], ylab="Log(AFH_activity)")
boxplot(X[,2], ylab="Log(AFH_antigen)")
```

```{r echo=FALSE}
group_1 <- data[data$Group == "Non-carrier",]

print("Summary and dimensions of the first group of individuals.")
summary(group_1)
dim(group_1)
group_2 <- data[data$Group == "Carrier",]

print("Summary and dimensions of the second group of individuals.")
summary(group_2)
dim(group_2)


X_carr <- as.matrix(cbind(group_2[2],group_2[3]))
X_noncarr <- as.matrix(cbind(group_1[2],group_1[3]))
```



## DATA ANALYSIS:
### Computing the LDA


```{r}
group_1 <- group_1[2:3]
group_2 <- group_2[2:3]

x1 <- group_1
x2 <- group_2


# compute sample mean vectors:

x1.mean <- colMeans(x1)
x2.mean <- colMeans(x2)
x1.mean
x2.mean

# compute pooled estimate for the covariance matrix:
#TODO check this formula again
S_carrier <- var(x2)
S_noncarrier <- var(x1)
S.u <- (44*(var(x1))+29*(var(x2)))/73
w <- solve(S.u)%*%(x1.mean-x2.mean)
w0 <- -(x1.mean+x2.mean)%*%w/2
ggplot(data, aes(x=AHF_activity, y=AHF_antigen, color=Group)) + geom_point() + geom_line(aes(X[,1], -(w[1]*X[,1]+w0)/w[2]))

```



# Partial code credit to Prof Li (UC Davis), Weiping Zhang(USTC)

## Two sample test with 
\(H_0 : \overrightarrow{\mu}_{noncarrier}\ = \overrightarrow{\mu}_{carrier}\) at level of \(\alpha\) = 0.05 with the Hotelling's \(T^2\) test.

```{r}
# now we perform the two-sample Hotelling T^2-test
n<-c(45,30)
p<-2
xmean1<-x1.mean
xmean2<-x2.mean
d<-xmean1-xmean2
d
Sp<-S.u
t2 <- t(d)%*%solve(sum(1/n)*Sp)%*%d
t2

alpha<-0.05
cval <- (sum(n)-2)*p/(sum(n)-p-1)*qf(1-alpha,p,sum(n)-p-1)
cval
```
## Since \(T^2\) = 95.16 > 6.33 the null hypothesis is rejected at 5% level of significance.


## Confidence Region for Non Carriers
```{r}
es<-eigen(sum(1/n)*Sp)
e1<-es$vec %*% diag(sqrt(es$val))
r1<-sqrt(cval)
theta<-seq(0,2*pi,len=250)
v1<-cbind(r1*cos(theta), r1*sin(theta))
pts<-t(xmean1-(e1%*%t(v1)))
plot(pts,type="l",main="Confidence Region for Bivariate Normal",xlab=expression(paste(mu, "(1)")), ylab=expression(paste(mu, "(2)")),asp=1)
segments(0,d[2],d[1],d[2],lty=2) # highlight the center
segments(d[1],0,d[1],d[2],lty=2)
#TODO check why these bars in the middle are weird and what are the values here corresponding to because we have the two groups to our variables
points(X)

th2<-c(0,pi/2,pi,3*pi/2,2*pi)   #adding the axis
v2<-cbind(r1*cos(th2), r1*sin(th2))
pts2<-t(d-(e1%*%t(v2)))
segments(pts2[3,1],pts2[3,2],pts2[1,1],pts2[1,2],lty=3)  
segments(pts2[2,1],pts2[2,2],pts2[4,1],pts2[4,2],lty=3)

# since we reject the null, we use the simultaneous confidence intervals
# to check the significant components
```


```{r}
es<-eigen(sum(1/n)*Sp)
e1<-es$vec %*% diag(sqrt(es$val))
r1<-sqrt(cval)
theta<-seq(0,2*pi,len=250)
v1<-cbind(r1*cos(theta), r1*sin(theta))
pts<-t(d-(e1%*%t(v1)))
plot(pts,type="l",main="Confidence Region for Bivariate Normal",xlab=expression(paste(mu, "(1)")), ylab=expression(paste(mu, "(2)")),asp=1)
segments(0,d[2],d[1],d[2],lty=2) # highlight the center
segments(d[1],0,d[1],d[2],lty=2)
#TODO check why these bars in the middle are weird and what are the values here corresponding to because we have the two groups to our variables
points(X)

th2<-c(0,pi/2,pi,3*pi/2,2*pi)   #adding the axis
v2<-cbind(r1*cos(th2), r1*sin(th2))
pts2<-t(d-(e1%*%t(v2)))
segments(pts2[3,1],pts2[3,2],pts2[1,1],pts2[1,2],lty=3)  
segments(pts2[2,1],pts2[2,2],pts2[4,1],pts2[4,2],lty=3)

# since we reject the null, we use the simultaneous confidence intervals
# to check the significant components
```

## Simultaneous confidence intervals
```{r}
wd<-sqrt(cval*diag(Sp)*sum(1/n))
Cis<-cbind(d-wd,d+wd)

# 95% simultaneous confidence interval
Cis
#plot(Cis[1][1]:0, 1:10, type="l", lty=2)
```
## Bonferroni simultaneous confidence intervals
```{r}
wd.b<- qt(1-alpha/(2*p),n[1]+n[2]-2) *sqrt(diag(Sp)*sum(1/n))
Cis.b<-cbind(d-wd.b,d+wd.b)
# 95% Bonferroni simultaneous confidence interval
Cis.b
# both component-wise simultaneous confidence intervals do not contain 0, so they have significant differences.
```

## CONCLUSION
### The company can use the following function to predict the amount of CPU time that they will need for a new set of parameters: 
- y = 1.07898250130454\(z_1\) + 0.419888473166963\(z_2\) + 8.42368896741667
  - \(z_1\)  is the value of the companies number of orders (in thousands) that they need to know CPU time requirements for
  - \(z_2\)  is the companies add-delete items (in thousands) that they need to know CPU time requirements for
  - y is the predicted CPU time needed. 

### Then, as generated in the last portion of the data analysis, we can see that if we want to be XX% (where an alpha value of 0.05 corresponds to 95%, alpha of 0.001 corresponds to 99.9% etc.) sure that the true value of CPU time will fall within the lower and upper bound we will add and subtract the values generated to the value of y predicted above for some value of \(z_{01}\) being the orders in thousands and \(z_{02}\) being the add-delete items in thousands. 
- Alpha value: 0.05  Interval: 3.91241721099891
- Alpha value: 0.01  Interval: 6.48784302704889
- Alpha value: 0.005  Interval: 7.88779247898333
- Alpha value: 0.001  Interval: 12.1331741930768

#### In addtion, these values can automatically be generated with the following script by just altering the line z_0 <- as.matrix(c(1,130,7.5)) by replacing "130 and 7.5 in the following manner. The general line would be z_0 <- as.matrix(c(1,\(z_{01}\),\(z_{02}\))) where the value of \(z_{01}\) is the orders in thousands and \(z_{02}\) is the add-delete items in thousands. 
```
for (val in c(0.05,0.01,0.005,0.001))
{
  z_0 <- as.matrix(c(1,130,7.5))
  z_0_beta_hat <- t(z_0)%*%beta_hat
  statistic<-qt(1-(val/2),n-r-1)
  ans<-t(z_0)%*%(solve(t(Z)%*%Z))%*%z_0
  interval <- sqrt(sigma_sq)*statistic*sqrt(1+ans)
  int_low <- z_0_beta_hat - interval
  int_up <- z_0_beta_hat + interval
  print (sprintf("Alpha value: %s  Interval: %s  Lower bound: %s  Upper bound: %s",  val, interval, int_low, int_up ))
}
```


### As a precaution we do suggest gather more data on this situation if possible. This will increase the strength of the analysis and account for other variables in the situation. This would be achieved by increasing the value of n, or observed situaitons (company CPU time requirements), and r, the number of observed varaibles upon which to make the prediction of CPU time (orders, add-delete items, etc.). Please contact if this is the case and a more robust script will be made to factor in variations in new potential data gathering.




---
<P style="page-break-before: always">
\newpage






---

# DATASET 3: PCA


## INTRODUCTION: 
### Data has been gatherd on populations which is considered "Census-tract data". The variables that have been gathered are "Total Population (thousands)", "Professional Degree (%)", "Employed age over 16 (%)", "Government Employment (%)", "Median home value ($100,000s)". 
1) Census data can be very important for a variety of reasons, one of the most important/common ones is predicting voting outcomes. 
2) Politicians may try to predict their popularity to certain populations by find the more common types of districts and try to gain popularity with one of those districts which would then hopefully have a similar effect on those other common areas. 
### PCA (principle component analysis) is a popular way to explore datasets with multiple dimensions due to the fact that it is a diminsional reduction technique which allows exploration of the data in 2 dimensions (depending on how much of the variance can be summarized in those two dimensions). This is a great high level method to explore which groups have the most variation and potentially cluster together; therefore, this data can help those interested, such as politicians, in understanding their demographic. 


## SUMMARIZE DATA:
### Lets summarize the dataframe
```{r echo=FALSE}
data <- read.table("T8-5.DAT", 
           header=FALSE)
colnames(data) <- c("Total_Population(thousands)", "Professional_Degree(%)", "Employed_age_over_16(%)", "Government_Employment(%)", "Median_home_value($100,000s)")
X <- as.matrix(cbind(data[,1], data[,2], data[,3], data[,4], data[,5]))
summary(data)
dim(data)
```
### Lets also look at some box plots to help visualize the distribution of our varibles
```{r echo=FALSE}
boxplot(data$`Total_Population(thousands)`, ylab="Total_Population(thousands)")
boxplot(data$`Professional_Degree(%)`, ylab="Professional_Degree(%)")
boxplot(data$`Employed_age_over_16(%)`, ylab="Employed_age_over_16(%)")
boxplot(data$`Government_Employment(%)`, ylab="Government_Employment(%)")
boxplot(data$`Median_home_value($100,000s)`, ylab="Median_home_value($100,000s)")
```

## DATA ANALYSIS:
### First lets take a look at the eigen values and eigen vectors of the covariance matrix. 
```{r echo=FALSE}
cov <- cov(X)
ev <- eigen(cov)
ev$values
ev$vectors
```

### Also, lets take a look at the eigen values and eigen vectors of the correlation matrix which is also sometimes a good way to summarize our data (can be preferrable if the primary eigen values are a larger proportion than that of the covariance matrix).
```{r echo=FALSE}
cor <- cor(X)
ev_cor <- eigen(cor)
ev_cor$values
ev_cor$vectors
```

### Lets see if the covariance matrix or the correlation matrix summarizes the data in the first two components better. 
```{r echo=FALSE}
print("Variance summarized in first two components of the covariance matrix:")
(ev$values[1]+ev$values[2])/sum(ev$values)
print("Variance summarized in first two components of the corrlation matrix:")
(ev_cor$values[1]+ev_cor$values[2])/sum(ev_cor$values)
```



## COMPARE TO THE STANDARDIZED MATRIX
```{r echo=FALSE}
X_stan <- scale(X)
X_stan
cov_stan <- cov(X_stan)
ev_stan <- eigen(cov_stan)
ev_stan$values
ev_stan$vectors
```

### Also, lets take a look at the eigen values and eigen vectors of the correlation matrix which is also sometimes a good way to summarize our data (can be preferrable if the primary eigen values are a larger proportion than that of the covariance matrix).
```{r echo=FALSE}
cor <- cor(X)
ev_cor <- eigen(cor)
ev_cor$values
ev_cor$vectors
```

### Lets see if the covariance matrix or the correlation matrix summarizes the data in the first two components better. 
```{r echo=FALSE}
print("Variance summarized in first two components of the covariance matrix:")
(ev$values[1]+ev$values[2])/sum(ev$values)
print("Variance summarized in first two components of the corrlation matrix:")
(ev_cor$values[1]+ev_cor$values[2])/sum(ev_cor$values)
```







### So we see that using the covariance matrix is preferred here.

### So since we can summarize 92% of the variance in our data using the first two principal components it would fair to graph our analysis using these two PCs. Lets also looks the the eigen value size graphed over the five components. Finally, we will overlay the PC scores for the sample data in the space of the first two principal components so we can visualize which of the sample data is most contributing to the the principal components which we see Government employment percentage and employed age over 16 (%) are the main two contributors to the 

#### TODO, how do we do this with stuff we have learned in the course. 
#### TODO, Do we need to do this for the standardized matrix instead?
```{r echo=FALSE}
state.pc <- princomp(data)
summary(state.pc, loadings = TRUE)
# Showing the eigenvalues of the covariance matrix:
(state.pc$sdev)^2
# A scree plot:
plot(1:(length(state.pc$sdev)),  (state.pc$sdev)^2, type='b', 
     main="Scree Plot", xlab="Number of Components", ylab="Eigenvalue Size")

# Plotting the PC scores for the sample data in the space of the first two principal components:
par(pty="s")
plot(state.pc$scores[,1], state.pc$scores[,2], 
     xlab="PC 1", ylab="PC 2", type ='n', lwd=2)
# labeling points with state abbreviations:
text(state.pc$scores[,1], state.pc$scores[,2], cex=0.7, lwd=2)
biplot(state.pc)
```

## CONCLUSION



### As a precaution we do suggest gather more data on this situation if possible. This will increase the strength of the analysis and account for other variables in the situation. This would be achieved by increasing the value of n, or observed situaitons (census data in new areas), and r, the number of observed varaibles upon which to build PCs (such as "Total_Population(thousands)", "Professional_Degree(%)", and "Employed_age_over_16(%)" etc). Please contact if this is the case and a more robust script will be made to factor in variations in new potential data gathering.

